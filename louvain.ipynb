{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "louvain.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jin-K-Yang/Clustering-by-Louvain-Algorithm/blob/main/louvain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow1H16iJB1cW",
        "outputId": "6a6e19e0-426a-4d4b-c770-3b118a64dd0f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB5aI8g1dazf"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPVErasgmPzB"
      },
      "source": [
        "import community as community_louvain\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "\n",
        "def louvain(filename):\n",
        "  # load one excel file\n",
        "  df = pd.read_excel(filename, usecols=['會員編號', '部門編號'], converters={'部門編號':str})\n",
        "  df = df.dropna(how='any',axis=0) \n",
        "\n",
        "  # convert into two array which represent customer and product\n",
        "  array = df.to_numpy()\n",
        "  split_array = np.hsplit(array, 2)\n",
        "  customer = split_array[0]\n",
        "  product = split_array[1]\n",
        "\n",
        "  # construct bipartite graph\n",
        "  G = nx.Graph()\n",
        "  for i in range(len(product)):\n",
        "      # zero padding\n",
        "      product[i][0] = product[i][0].zfill(5)\n",
        "\n",
        "      # add nodes\n",
        "      G.add_nodes_from(product[i], bipartite = 0)\n",
        "      G.add_nodes_from(customer[i], bipartite = 1)\n",
        "      \n",
        "      # add edges\n",
        "      if G.has_edge(product[i][0], customer[i][0]):\n",
        "          G[product[i][0]][customer[i][0]]['weight'] += 1\n",
        "      else:\n",
        "          G.add_edge(product[i][0], customer[i][0], weight = 1)\n",
        "\n",
        "  # show the total number of nodes and edges\n",
        "  # print(G.number_of_nodes())\n",
        "  # print(G.number_of_edges())\n",
        "\n",
        "  # compute the best partition\n",
        "  partition = community_louvain.best_partition(G)\n",
        "  # print(partition)\n",
        "\n",
        "  # classify the result\n",
        "  community = {}\n",
        "  for key, value in set(partition.items()):\n",
        "      if key[0].isalpha():\n",
        "          community.setdefault(value, {}).setdefault('customer', []).append(key)\n",
        "      else:\n",
        "          community.setdefault(value, {}).setdefault('product', []).append(key)\n",
        "\n",
        "  # calculate the sum of edges of clusters\n",
        "  for i in range(len(community)):\n",
        "    SG = G.subgraph(community[i]['product'] + community[i]['customer'])\n",
        "    community[i]['edges_sum'] = SG.number_of_edges()\n",
        "  return community\n",
        "  \"\"\"\n",
        "  total_name=''\n",
        "  for i in range(len(community)):\n",
        "    print(i)\n",
        "    for j in community[i]['product']:\n",
        "      total_name += num_to_product(j)\n",
        "      total_name += ' , '\n",
        "    print(total_name)\n",
        "    total_name=''\n",
        "  \"\"\"\n",
        "\n",
        "# folder version\n",
        "def louvain_folder(filename):\n",
        "  # load multiple excel file\n",
        "  files = glob(filename)\n",
        "  df = pd.concat([pd.read_excel(f, usecols=['會員編號', '部門編號'], converters={'部門編號':str}) for f in files])\n",
        "  df.dropna(how='any',axis=0)\n",
        "  df.reset_index(drop=True)\n",
        "\n",
        "  # convert into two array which represent customer and product\n",
        "  array = df.to_numpy()\n",
        "  split_array = np.hsplit(array, 2)\n",
        "  customer = split_array[0]\n",
        "  product = split_array[1]\n",
        "\n",
        "  # construct bipartite graph\n",
        "  G = nx.Graph()\n",
        "  for i in range(len(product)):\n",
        "      # zero padding\n",
        "      product[i][0] = product[i][0].zfill(5)\n",
        "\n",
        "      # add nodes\n",
        "      G.add_nodes_from(product[i], bipartite = 0)\n",
        "      G.add_nodes_from(customer[i], bipartite = 1)\n",
        "      \n",
        "      # add edges\n",
        "      if G.has_edge(product[i][0], customer[i][0]):\n",
        "          G[product[i][0]][customer[i][0]]['weight'] += 1\n",
        "      else:\n",
        "          G.add_edge(product[i][0], customer[i][0], weight = 1)\n",
        "\n",
        "  # show the total number of nodes and edges\n",
        "  # print(G.number_of_nodes())\n",
        "  # print(G.number_of_edges())\n",
        "\n",
        "  # compute the best partition\n",
        "  partition = community_louvain.best_partition(G)\n",
        "\n",
        "  # classify the result\n",
        "  community = {}\n",
        "  for key, value in set(partition.items()):\n",
        "      if key[0].isalpha():\n",
        "          community.setdefault(value, {}).setdefault('customer', []).append(key)\n",
        "      else:\n",
        "          community.setdefault(value, {}).setdefault('product', []).append(key)\n",
        "\n",
        "  # calculate the sum of edges of clusters\n",
        "  for i in range(len(community)):\n",
        "    SG = G.subgraph(community[i]['product'] + community[i]['customer'])\n",
        "    community[i]['edges_sum'] = SG.number_of_edges()\n",
        "  return community\n",
        "\n",
        "# benefit-cost analysis\n",
        "def benefit_cost(community):\n",
        "  benefit = {\n",
        "      'Customer_Count':[],\n",
        "      'Product_Count':[],\n",
        "      'Cost':[],\n",
        "      'Benefit':[],\n",
        "      'Ratio':[]\n",
        "  }\n",
        "\n",
        "  total = {\n",
        "      'Customer_Count':0,\n",
        "      'Product_Count':0,\n",
        "      'Cost':0,\n",
        "      'Benefit':0,\n",
        "      'Ratio':0\n",
        "  }\n",
        "\n",
        "  for i in range(len(community)):\n",
        "      benefit['Customer_Count'].append(len(community[i]['customer']))\n",
        "      benefit['Product_Count'].append(len(community[i]['product']))\n",
        "      benefit['Cost'].append(len(community[i]['customer']) * len(community[i]['product']))\n",
        "      benefit['Benefit'].append(community[i]['edges_sum'])\n",
        "      benefit['Ratio'].append(community[i]['edges_sum'] / (len(community[i]['customer']) * len(community[i]['product'])))\n",
        "\n",
        "      total['Customer_Count'] += len(community[i]['customer'])\n",
        "      total['Product_Count'] += len(community[i]['product'])\n",
        "      total['Cost'] += len(community[i]['customer']) * len(community[i]['product'])\n",
        "      total['Benefit'] += community[i]['edges_sum']\n",
        "\n",
        "  total['Ratio'] = total['Benefit'] / total['Cost']\n",
        "\n",
        "  df_total = pd.DataFrame(total, index=['total'])\n",
        "  df_benefit = pd.DataFrame(benefit)\n",
        "\n",
        "  result = pd.concat([df_benefit, df_total])\n",
        "  return result\n",
        "\n",
        "# map the department number and deparment name from excel file\n",
        "def map_department(file):\n",
        "  map_df = pd.read_excel(file, usecols=['部門名稱', '部門編號'], converters={'部門編號':str})\n",
        "  map_array = np.hsplit(map_df.to_numpy(), 2)\n",
        "  map = {}\n",
        "\n",
        "  for i in range(len(map_array[0])):\n",
        "    map[map_array[0][i][0].zfill(5)] = map_array[1][i][0]\n",
        "  return map\n",
        "\n",
        "# show the product name in each clusters\n",
        "def show_product(map, community):\n",
        "  for i in range(len(community)):\n",
        "    for j in range(len(community[i]['product'])):\n",
        "      community[i]['product'][j] = map[community[i]['product'][j]]\n",
        "    print(community[i]['product'])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X3p4SF3LRZB"
      },
      "source": [
        "result_2018 = louvain('/content/drive/Shareddrives//louvain/year/2018.xlsx')\n",
        "benefit_cost_2018 = benefit_cost(result_2018)\n",
        "benefit_cost_2018"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qiafgdcfjYu"
      },
      "source": [
        "result_2019 = louvain('/content/drive/Shareddrives//louvain/year/2019.xlsx')\n",
        "benefit_cost_2019 = benefit_cost(result_2019)\n",
        "benefit_cost_2019"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yn7juCHOFGK"
      },
      "source": [
        "result_2020 = louvain('/content/drive/Shareddrives//louvain/year/2020.xlsx')\n",
        "benefit_cost_2020 = benefit_cost(result_2020)\n",
        "benefit_cost_2020"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz8U_y9Ecx_L"
      },
      "source": [
        "map = map_department('/content/drive/Shareddrives//louvain/department_list.xlsx')\n",
        "show_product(map, result_2018)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEugro94ctib"
      },
      "source": [
        "map = map_department('/content/drive/Shareddrives//louvain/department_list.xlsx')\n",
        "show_product(map, result_2019)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGJt96ZkCNPx"
      },
      "source": [
        "map = map_department('/content/drive/Shareddrives//louvain/department_list.xlsx')\n",
        "show_product(map, result_2020)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjLa3U0lYRGM"
      },
      "source": [
        "# write result to the excel file\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "def write_clusters(file_name, sheetname, result):\n",
        "  wb = load_workbook(filename = file_name)\n",
        "  sheet = wb[sheetname]\n",
        "  for i in range(len(result)):\n",
        "    for j in range(len(result[i]['product'])):\n",
        "      sheet.cell(row=i+2, column=7+j).value = result[i]['product'][j]\n",
        "  wb.save(file_name)\n",
        "\n",
        "with pd.ExcelWriter('/content/drive/Shareddrives//louvain/total_result.xlsx') as writer:  \n",
        "  benefit_cost_2018.to_excel(writer, sheet_name='2018')\n",
        "  benefit_cost_2019.to_excel(writer, sheet_name='2019')\n",
        "  benefit_cost_2020.to_excel(writer, sheet_name='2020')\n",
        "\n",
        "write_clusters('/content/drive/Shareddrives//louvain/total_result.xlsx', '2018', result_2018)\n",
        "write_clusters('/content/drive/Shareddrives//louvain/total_result.xlsx', '2019', result_2019)\n",
        "write_clusters('/content/drive/Shareddrives//louvain/total_result.xlsx', '2020', result_2020)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "o1xzv_AhJxZK",
        "outputId": "797c735c-b570-47cc-8123-fbb5edd7c9a0"
      },
      "source": [
        "#compare each clusters' people if they are same and draw the Venn diagram\n",
        "from matplotlib_venn import venn2, venn2_circles, venn2_unweighted\n",
        "from matplotlib_venn import venn3, venn3_circles\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def compare_two_years(cluster_1, cluster_2):\n",
        "  count = 0\n",
        "  for i in range(len((cluster_1['customer']))):\n",
        "    for j in range(len(cluster_2['customer'])):\n",
        "      if cluster_1['customer'][i] == cluster_2['customer'][j]:\n",
        "        count += 1\n",
        "  return count\n",
        "\n",
        "def compare_three_years(cluster_1, cluster_2, cluster_3):\n",
        "  count = 0\n",
        "  for i in range(len((cluster_1['customer']))):\n",
        "    for j in range(len(cluster_2['customer'])):\n",
        "      if cluster_1['customer'][i] == cluster_2['customer'][j]:\n",
        "        for k in range(len(cluster_3['customer'])):\n",
        "          if cluster_1['customer'][i] == cluster_3['customer'][k]:\n",
        "            count += 1\n",
        "  return count\n",
        "\n",
        "def draw_Venn_diagram(cluster_1, cluster_2, cluster_3):\n",
        "  venn3(subsets = (len(cluster_1['customer']), len(cluster_2['customer']), compare_two_years(cluster_1, cluster_2), len(cluster_3['customer']), compare_two_years(cluster_1, cluster_3), compare_two_years(cluster_2, cluster_3), compare_three_years(cluster_1, cluster_2, cluster_3)), set_labels = ('2018', '2019', '2020'))\n",
        "\n",
        "draw_Venn_diagram(result_2018[10], result_2019[17], result_2020[19])\n",
        "plt.title('cluster');"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qn9N4I9tOsq"
      },
      "source": [
        "# draw the graph\n",
        "pos = nx.spring_layout(G)\n",
        "\n",
        "# color the nodes according to their partition\n",
        "cmap = cm.get_cmap('viridis', max(partition.values()) + 1)\n",
        "nx.draw_networkx_nodes(G, pos, partition.keys(), node_size=1, cmap=cmap, node_color=list(partition.values()))\n",
        "nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNFGK3XadlVs",
        "outputId": "af28639f-cfd9-40c8-f5b4-f5ebe883f667"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import timedelta\n",
        "from matplotlib_venn import venn2\n",
        "from matplotlib_venn import venn2_circles\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import LinearSVC\n",
        "import time\n",
        "\n",
        "\n",
        "class RFM:\n",
        "    def __init__(self, i_file, p_e_date, \n",
        "                 p_s_date='default', t_s_date='default', bucket_num=100, recency_ascending=True):\n",
        "        self.i_file = i_file\n",
        "        self.date_format = '%Y%m%d'\n",
        "        self.bucket_num = bucket_num\n",
        "        self.recency_ascending = recency_ascending\n",
        "        # 讀取檔案\n",
        "        self.df = pd.read_csv(self.i_file, usecols=['會員編號', '交易日期', '交易金額'])\n",
        "        # 從原始欄位中篩選所要的欄位，並重新命名成英文欄位名稱\n",
        "        self.df = self.df.loc[:, ['會員編號', '交易日期', '交易金額']]\n",
        "        self.df.columns = ['ID', 'T_DATE', 'AMOUNT']\n",
        "        # 將日期改成日期格式\n",
        "        self.df['T_DATE'] = pd.to_datetime(self.df['T_DATE'], format=self.date_format)\n",
        "        # 先排序交易日期，後排序會員編號\n",
        "        self.df.sort_values(by=['ID', 'T_DATE'])\n",
        "        # 2. 切割訓練資料、回購標記與驗證資料\n",
        "        # #### 資料期間 ######\n",
        "        self.data_start_date = self.df['T_DATE'].sort_values().iloc[1]  \n",
        "        self.data_end_date = self.df['T_DATE'].sort_values().iloc[-1]  # 擁有資料中的最後日期\n",
        "        self.data_duration = self.data_end_date - self.data_start_date + timedelta(days=1)\n",
        "        # #### 預測期間 ######\n",
        "        if p_s_date == 'default':\n",
        "            self.predict_start_date = self.data_end_date + timedelta(days=1)\n",
        "        else:\n",
        "            self.predict_start_date = pd.Timestamp(p_s_date)\n",
        "        self.predict_end_date = pd.Timestamp(p_e_date)\n",
        "        self.predict_duration = self.predict_end_date - self.predict_start_date + timedelta(days=1)\n",
        "        # #### 訓練期間 ######\n",
        "        if t_s_date == 'default':\n",
        "            self.train_start_date = self.data_start_date\n",
        "        else:\n",
        "            self.train_start_date = pd.Timestamp(t_s_date)\n",
        "        self.train_end_date = self.predict_start_date - self.predict_duration - timedelta(days=1)\n",
        "        self.train_duration = self.train_end_date - self.train_start_date + timedelta(days=1)\n",
        "        # #### 回購標記 ######\n",
        "        self.repurchase_start_date = self.train_end_date + timedelta(days=1)\n",
        "        self.repurchase_end_date = self.train_end_date + self.predict_duration\n",
        "        # #### 驗證期間 ######\n",
        "        self.test_start_date = self.train_start_date + self.predict_duration\n",
        "        self.test_end_date = self.test_start_date + self.train_duration - timedelta(days=1)\n",
        "        \n",
        "    @staticmethod\n",
        "    def regen_rfm(df1, df2, bucket_num = 100):\n",
        "        df2.R = df1.R.max() + df2.R\n",
        "        basis_data = pd.concat([df1, df2], ignore_index=True)\n",
        "        # 計算RFM值\n",
        "        Pred = pd.DataFrame()\n",
        "        Pred['R'] = basis_data.groupby('ID')['R'].max()\n",
        "        Pred['F'] = basis_data.groupby('ID')['F'].sum()\n",
        "        Pred['M'] = basis_data.groupby('ID')['M'].sum()\n",
        "        Pred['ID'] = Pred.index\n",
        "        Pred = Pred[['ID', 'R', 'F', 'M']]\n",
        "        # 產生BUCKET值\n",
        "        rows = len(Pred.index)\n",
        "        data = np.ones(rows, dtype=np.int)\n",
        "        s = pd.Series(data)\n",
        "        bucket_size = int(rows/bucket_num)+1\n",
        "        for i in range(bucket_num-1):\n",
        "            s.iloc[bucket_size*i:bucket_size*(i+1)] = bucket_num-i\n",
        "        Pred = Pred.sort_values(by=['R', 'ID'], ascending=[False, True])\n",
        "        Pred['RB'] = s.values\n",
        "        Pred = Pred.sort_values(by=['F', 'R', 'ID'], ascending=[False, False, True])\n",
        "        Pred['FB'] = s.values\n",
        "        Pred = Pred.sort_values(by=['M', 'F', 'R', 'ID'],\n",
        "                                    ascending=[False, False, False, True])\n",
        "        Pred['MB'] = s.values\n",
        "        Pred = Pred.sort_values(by='ID')\n",
        "        return Pred\n",
        "    \n",
        "    def get_info(self, o_file='default'):\n",
        "        sort = 'desc'\n",
        "        if self.recency_ascending:\n",
        "            sort = 'asc'\n",
        "        if o_file == 'default':\n",
        "            o_file = '{:0>4d}{:0>2d}{:0>2d}_{:0>4d}{:0>2d}{:0>2d}-{:0>2d}{:0>2d}_{}_info.csv'.format(\n",
        "                        self.train_start_date.year, self.train_start_date.month, self.train_start_date.day,\n",
        "                        self.predict_start_date.year, self.predict_start_date.month, self.predict_start_date.day,\n",
        "                        self.predict_end_date.month, self.predict_end_date.day, sort)\n",
        "        info = pd.DataFrame()\n",
        "        info['開始日期'] = [self.data_start_date, self.train_start_date, self.repurchase_start_date,\n",
        "                        self.test_start_date, self.predict_start_date]\n",
        "        info['結束日期'] = [self.data_end_date, self.train_end_date, self.repurchase_end_date,\n",
        "                        self.test_end_date, self.predict_end_date]\n",
        "        info['計算天數'] = (info['結束日期'] - info['開始日期'] + timedelta(days=1)).apply(lambda x: x.days)\n",
        "\n",
        "        def count_member(d1, d2):\n",
        "            mask1 = self.df['T_DATE'] >= d1\n",
        "            mask2 = self.df['T_DATE'] <= d2\n",
        "            return self.df[mask1 & mask2].ID.nunique()\n",
        "        info['交易人數'] = info.apply(lambda x: count_member(x['開始日期'], x['結束日期']), axis=1)\n",
        "        info.index = ['資料期間', '訓練期間', '回購標記', '驗證期間', '預測期間']\n",
        "        # 輸出檔案\n",
        "        info.to_csv(o_file)\n",
        "        return o_file\n",
        "    # 處理訓練資料\n",
        "        \n",
        "    def gen_train(self, o_file='default', oversampling=4, features=['R', 'F', 'M']):\n",
        "        sort = 'desc'\n",
        "        if self.recency_ascending:\n",
        "            sort = 'asc'\n",
        "        if o_file == 'default':\n",
        "            o_file = '{:0>4d}{:0>2d}{:0>2d}_{:0>4d}{:0>2d}{:0>2d}-{:0>2d}{:0>2d}_{}_train_o{}.csv'.format(\n",
        "                        self.train_start_date.year, self.train_start_date.month, self.train_start_date.day,\n",
        "                        self.predict_start_date.year, self.predict_start_date.month, self.predict_start_date.day,\n",
        "                        self.predict_end_date.month, self.predict_end_date.day, sort, oversampling)\n",
        "        mask1 = self.df['T_DATE'] >= self.train_start_date\n",
        "        mask2 = self.df['T_DATE'] <= self.train_end_date\n",
        "        data = self.df[mask1 & mask2]\n",
        "        result = self.gen_rfm(data, 'train', features)\n",
        "        # 計算REPURCHASE值\n",
        "        # 找出回購標記範圍\n",
        "        mask1 = self.df['T_DATE'] >= self.repurchase_start_date\n",
        "        mask2 = self.df['T_DATE'] <= self.repurchase_end_date\n",
        "        repurchase = self.df[mask1 & mask2]\n",
        "        result['REPURCHASE'] = pd.Series(result.index.isin(repurchase['ID'])).astype('int').values\n",
        "        repurchase_success = result[result.REPURCHASE == 1]\n",
        "        # Oversampling\n",
        "        if oversampling > 0:\n",
        "            for _ in range(oversampling-1):\n",
        "                result = result.append(repurchase_success, ignore_index=True)\n",
        "        # 按照ID作排序\n",
        "        result = result.sort_values(by=['ID'], ascending=[True])\n",
        "        # 輸出檔案\n",
        "        result.to_csv(o_file, index=False)\n",
        "        return o_file\n",
        "    \n",
        "    # 處理驗證資料\n",
        "    def gen_val(self, o_file='default', features=['R', 'F', 'M']):\n",
        "        sort = 'desc'\n",
        "        if self.recency_ascending:\n",
        "            sort = 'asc'\n",
        "        if o_file == 'default':\n",
        "            o_file = '{:0>4d}{:0>2d}{:0>2d}_{:0>4d}{:0>2d}{:0>2d}-{:0>2d}{:0>2d}_{}_val.csv'.format( \n",
        "                        self.train_start_date.year, self.train_start_date.month, self.train_start_date.day,\n",
        "                        self.predict_start_date.year, self.predict_start_date.month, self.predict_start_date.day,\n",
        "                        self.predict_end_date.month, self.predict_end_date.day, sort)\n",
        "        mask1 = self.df['T_DATE'] >= self.test_start_date\n",
        "        mask2 = self.df['T_DATE'] <= self.test_end_date\n",
        "        data = self.df[mask1 & mask2]\n",
        "        result = self.gen_rfm(data, 'val', features)\n",
        "        # 計算REPURCHASE值\n",
        "        # 找出回購標記範圍\n",
        "        mask1 = self.df['T_DATE'] >= self.predict_start_date\n",
        "        mask2 = self.df['T_DATE'] <= self.predict_end_date\n",
        "        repurchase = self.df[mask1 & mask2]\n",
        "        result['REPURCHASE'] = pd.Series(result.index.isin(repurchase['ID'])).astype('int').values\n",
        "        repurchase_amt = repurchase.groupby('ID')['AMOUNT'].sum()\n",
        "        result = result.merge(repurchase_amt.to_frame(), on='ID', how='left')\n",
        "        # 按照ID作排序\n",
        "        result = result.sort_values(by=['ID'], ascending=[True])\n",
        "        # 輸出檔案\n",
        "        result.to_csv(o_file, index=False)\n",
        "        return o_file\n",
        "        # 處理測試資料\n",
        "\n",
        "    def gen_test(self, o_file='default', features=['R', 'F', 'M']):\n",
        "        sort = 'desc'\n",
        "        if self.recency_ascending:\n",
        "            sort = 'asc'\n",
        "        if o_file == 'default':\n",
        "            o_file = '{:0>4d}{:0>2d}{:0>2d}_{:0>4d}{:0>2d}{:0>2d}-{:0>2d}{:0>2d}_{}_test.csv'.format( \n",
        "                        self.train_start_date.year, self.train_start_date.month, self.train_start_date.day,\n",
        "                        self.predict_start_date.year, self.predict_start_date.month, self.predict_start_date.day,\n",
        "                        self.predict_end_date.month, self.predict_end_date.day, sort)\n",
        "        mask1 = self.df['T_DATE'] >= self.test_start_date\n",
        "        mask2 = self.df['T_DATE'] <= self.test_end_date\n",
        "        data = self.df[mask1 & mask2]\n",
        "        result = self.gen_rfm(data, 'test', features)\n",
        "        # 按照ID作排序\n",
        "        result = result.sort_values(by=['ID'], ascending=[True])\n",
        "        # 輸出檔案\n",
        "        result.to_csv(o_file, index=False)\n",
        "        return o_file\n",
        "\n",
        "    def gen_rfm(self, data, date_for='train', features=['R', 'F', 'M']):\n",
        "        time_stamp = time.time()\n",
        "        # 將時間更改成相同基底\n",
        "        def change_r_basis(x):\n",
        "            if date_for == 'train':\n",
        "                if self.recency_ascending:  # 奇廷日期基數校正\n",
        "                    return (x - self.train_start_date + timedelta(days=1)).days\n",
        "                else:                       # 製圖用倒推日期基數\n",
        "                    return (self.train_end_date - x + timedelta(days=1)).days\n",
        "            else:  # date_for='val', date_for='test'\n",
        "                if self.recency_ascending:  # 奇廷日期基數校正\n",
        "                    return (x - self.test_start_date + timedelta(days=1)).days\n",
        "                else:                       # 製圖用倒推日期基數\n",
        "                    return (self.test_end_date - x + timedelta(days=1)).days\n",
        "\n",
        "        def id_trans(id_str):  # 取代全型字體為半型\n",
        "            return id_str.translate(str.maketrans('０１２３４５６７８９', '0123456789'))\n",
        "        \n",
        "        basis_data = data.copy()\n",
        "        basis_data['T_DATE'] = data['T_DATE'].apply(change_r_basis).copy()\n",
        "        basis_data['ID'] = data['ID'].apply(id_trans).copy()\n",
        "        # 計算RFM值\n",
        "        result = pd.DataFrame()\n",
        "        \n",
        "        if self.recency_ascending:  # 奇廷日期基數校正\n",
        "            result['R'] = basis_data.groupby('ID')['T_DATE'].max()\n",
        "        else:\n",
        "            result['R'] = basis_data.groupby('ID')['T_DATE'].min()\n",
        "        if features.count('L') > 0: \n",
        "            result['FR'] = basis_data.groupby('ID')['T_DATE'].min()\n",
        "            result['L'] = result['R'] - result['FR']\n",
        "        result['F'] = basis_data.groupby('ID')['T_DATE'].nunique()  # 取代原本奇廷day merge的作法\n",
        "        result['M'] = basis_data.groupby('ID')['AMOUNT'].sum()\n",
        "\n",
        "        # 新增CAI算法     \n",
        "        def get_avg_p(series):\n",
        "            df = pd.Series(series.unique())\n",
        "            if len(df)==1:\n",
        "                return 0\n",
        "            df = df.sort_values()\n",
        "            df = df.diff().dropna()\n",
        "            return df.mean()\n",
        "        def get_wei_p(series):\n",
        "            df = pd.Series(series.unique())\n",
        "            if len(df)==1:\n",
        "                return 0\n",
        "            df = df.sort_values()\n",
        "            df = df.diff().dropna()\n",
        "            df_rows = len(df)\n",
        "            df.index = np.arange(1, df_rows + 1)\n",
        "            return (df.index*df).sum()/((df_rows+df_rows*df_rows)/2)\n",
        "        if features.count('CAI') > 0 or features.count('NPT') > 0:\n",
        "            result['AVG_P'] = basis_data.groupby('ID')['T_DATE'].apply(get_avg_p)\n",
        "        if features.count('CAI') > 0 or features.count('WNPT') > 0:\n",
        "            result['WEI_P'] = basis_data.groupby('ID')['T_DATE'].apply(get_wei_p)\n",
        "        if features.count('CAI') > 0:\n",
        "            result['CAI'] = 1 - result['WEI_P']/result['AVG_P']\n",
        "            result['CAI'] = result['CAI'].fillna(0)\n",
        "        if features.count('NPT') > 0:    \n",
        "            result['NPT'] = result['R'] + result['AVG_P']\n",
        "        if features.count('WNPT') > 0: \n",
        "            result['WNPT']  = result['R'] + result['WEI_P']\n",
        "\n",
        "        # 新增CCC算法\n",
        "        def get_ccc(g):\n",
        "            r = g.T_DATE.max()\n",
        "            m1 = g[g.T_DATE > r-30].AMOUNT.sum()\n",
        "            m6 = g[g.T_DATE > r-180].AMOUNT.sum()/6\n",
        "            return m1/m6\n",
        "        if features.count('CCC') > 0:\n",
        "            result['CCC'] = basis_data.groupby('ID').apply(get_ccc)\n",
        "        \n",
        "        result['ID'] = result.index\n",
        "        result.index.name = 'Index'\n",
        "        result = result[['ID']+features]\n",
        "        # 產生BUCKET值\n",
        "        rows = len(result.index)\n",
        "        data = np.ones(rows, dtype=np.int)\n",
        "        s = pd.Series(data)\n",
        "        bucket_size = int(rows/self.bucket_num)+1\n",
        "        for i in range(self.bucket_num-1):\n",
        "            s.iloc[bucket_size*i:bucket_size*(i+1)] = self.bucket_num-i\n",
        "        result = result.sort_values(by=['R', 'ID'], ascending=[not self.recency_ascending, True])\n",
        "        result['RB'] = s.values\n",
        "        result = result.sort_values(by=['F', 'R', 'ID'], ascending=[False, not self.recency_ascending, True])\n",
        "        result['FB'] = s.values\n",
        "        result = result.sort_values(by=['M', 'F', 'R', 'ID'],\n",
        "                                    ascending=[False, False, not self.recency_ascending, True])\n",
        "        result['MB'] = s.values\n",
        "        result = result.sort_values(by='ID')\n",
        "        return result\n",
        "    \n",
        "class Forest:  \n",
        "    @staticmethod\n",
        "    def gen_result(train_file, test_file, o_file='default', seed=0, features=['R', 'F', 'M', 'RB', 'FB', 'MB']):\n",
        "        if o_file == 'default':\n",
        "            o_file = train_file.replace('train', 'result')\n",
        "        result = Forest.get_result(train_file, test_file, seed, features)\n",
        "        result.to_csv(o_file, index=False)\n",
        "        return o_file\n",
        "    @staticmethod\n",
        "    def get_result(train_file, test_file, seed=0, features=['R', 'F', 'M', 'RB', 'FB', 'MB']):\n",
        "        train = pd.read_csv(train_file)\n",
        "        test = pd.read_csv(test_file)\n",
        "\n",
        "        Train = train.loc[:, features]\n",
        "        label = train.loc[:, ['REPURCHASE']]\n",
        "        Test = test.loc[:,  features]\n",
        "        Result = test.loc[:,  ['ID']+features]\n",
        "        d = len(features) #特徵值數量==樹深度\n",
        "        \n",
        "        clf = RandomForestClassifier(n_estimators=30, max_depth=d, min_samples_leaf=2, random_state=seed,\n",
        "                                     criterion='entropy')\n",
        "        clf.fit(Train, label.values.ravel())\n",
        "        p = pd.DataFrame(clf.predict_proba(Test))\n",
        "        pred = p.iloc[:, 1]\n",
        "        Result['Pred'] = pred\n",
        "        return Result    \n",
        "    \n",
        "class Naive:\n",
        "    @staticmethod\n",
        "    def get_result(train_file, test_file, features=['R', 'F', 'M', 'RB', 'FB', 'MB']):\n",
        "        train = pd.read_csv(train_file)\n",
        "        test = pd.read_csv(test_file)\n",
        "\n",
        "        Train = train.loc[:, features]\n",
        "        label = train.loc[:, ['REPURCHASE']]\n",
        "        Test = test.loc[:,  features]\n",
        "        Result = test.loc[:,  ['ID']+features]\n",
        "\n",
        "        clf = GaussianNB()\n",
        "        clf.fit(Train, label.values.ravel())\n",
        "        p = pd.DataFrame(clf.predict_proba(Test))\n",
        "        pred = p.iloc[:, 1]\n",
        "        Result['Pred'] = pred\n",
        "        return Result\n",
        "    \n",
        "class SVM:\n",
        "    @staticmethod\n",
        "    def get_result(train_file, test_file, features=['R', 'F', 'M', 'RB', 'FB', 'MB']):\n",
        "        train = pd.read_csv(train_file)\n",
        "        test = pd.read_csv(test_file)\n",
        "\n",
        "        Train = train.loc[:, features]\n",
        "        label = train.loc[:, ['REPURCHASE']]\n",
        "        Test = test.loc[:,  features]\n",
        "        Result = test.loc[:,  ['ID']+features]\n",
        "\n",
        "        clf = LinearSVC(random_state=1, tol=1e-5)\n",
        "        clf.fit(Train, label.values.ravel())\n",
        "        Result['Pred'] = clf.predict(Test)\n",
        "        return Result    \n",
        "    \n",
        "class Diagram:\n",
        "    \n",
        "    @staticmethod\n",
        "    def draw_r_diagram(val_file, axes, title, y_max='default'):\n",
        "        df = pd.read_csv(val_file)\n",
        "        r_max = np.max(df.R)\n",
        "        df.R = r_max - df.R + 1  # 轉成desc\n",
        "        r_max = 36*30  # 觀察三年\n",
        "        b = np.arange(1, r_max+30, 30)   \n",
        "        hist = df[df.REPURCHASE == 1].R.hist(bins=b, label='Repurchase Customer', ax=axes, color='darkorange')\n",
        "        fd = {'fontsize': 18}\n",
        "        hist.set_title(title, fontdict=fd)\n",
        "        fd = {'fontsize': 16}\n",
        "        hist.set_xlabel('Recency (last month)', fontdict=fd)\n",
        "        hist.set_ylabel('Customers', fontdict=fd)\n",
        "        hist.legend()\n",
        "        hist.set_xticks(np.arange(1, r_max+60, 60))  # 以兩個月為單位\n",
        "        hist.set_xticklabels(np.arange(0, int((r_max+60)/30), 2))\n",
        "        hist.set_xlim(0, r_max)\n",
        "        if not y_max == 'default':\n",
        "            hist.set_ylim(0, y_max)\n",
        "        count1, division1 = np.histogram(df[df.REPURCHASE == 1].R, bins=b)\n",
        "        df_d = pd.DataFrame({'Repurchase Customer': count1})\n",
        "        df_transpose = df_d.T\n",
        "        return df_transpose\n",
        "\n",
        "    @staticmethod\n",
        "    def draw_p_r_diagram(result_file, th, axes, title, y_max='default'):\n",
        "        df = pd.read_csv(result_file)\n",
        "        r_max = np.max(df.R)\n",
        "        df.R = r_max - df.R + 1  # 轉成desc\n",
        "        r_max = 36*30  # 觀察三年\n",
        "        b = np.arange(1, r_max+30, 30)   \n",
        "        hist = df[df['Pred'] >= th].R.hist(bins=b, \n",
        "                                           label='Repurchase Customer(Threshold=={})'.format(th),\n",
        "                                           ax=axes, color='magenta')\n",
        "        fd = {'fontsize': 18}\n",
        "        hist.set_title(title, fontdict=fd)\n",
        "        fd = {'fontsize': 16}\n",
        "        hist.set_xlabel('Recency (last month)', fontdict=fd)\n",
        "        hist.set_ylabel('Customers', fontdict=fd)\n",
        "        hist.legend()\n",
        "        hist.set_xticks(np.arange(1, r_max+60, 60))  # 以兩個月為單位\n",
        "        hist.set_xticklabels(np.arange(0, int((r_max+60)/30), 2))\n",
        "        hist.set_xlim(0, r_max)\n",
        "        if not y_max == 'default':\n",
        "            hist.set_ylim(0, y_max)\n",
        "        count1, division1 = np.histogram(df[df['Pred'] >= th].R, bins=b)\n",
        "        df_d = pd.DataFrame({'Repurchase Customer': count1})\n",
        "        df_transpose = df_d.T\n",
        "        return df_transpose\n",
        "\n",
        "    @staticmethod\n",
        "    def draw_f_diagram(val_file, axes, title):\n",
        "        df = pd.read_csv(val_file)\n",
        "        b = np.arange(1, 31, 1)    \n",
        "        hist = df[(df.REPURCHASE == 0) & (df.F <= 30)].F.hist(bins=b, label='Non-Repurchase Customer',\n",
        "                                                              ax=axes, color='blue')\n",
        "        df[(df.REPURCHASE == 1) & (df.F <= 30)].F.hist(bins=b, label='Repurchase Customer',\n",
        "                                                       ax=axes, color='darkorange')\n",
        "        fd = {'fontsize': 18}\n",
        "        hist.set_title(title, fontdict=fd)\n",
        "        fd = {'fontsize': 16}\n",
        "        hist.set_xlabel('Frequecy', fontdict=fd)\n",
        "        hist.set_ylabel('Customers', fontdict=fd)\n",
        "        hist.legend()\n",
        "        hist.set_xlim(1, 31)\n",
        "        hist.set_xticks(np.arange(1, 32, 5))\n",
        "        hist.set_xticklabels(np.arange(0, 31, 5))\n",
        "        count1, division1 = np.histogram(df[(df.REPURCHASE == 0) & (df.F <= 30)].F, bins=b)\n",
        "        count2, division2 = np.histogram(df[(df.REPURCHASE == 1) & (df.F <= 30)].F, bins=b)\n",
        "        df_d = pd.DataFrame({'Non-Repurchase Customer': count1, 'Repurchase Customer': count2})\n",
        "        df_transpose = df_d.T\n",
        "        return df_transpose\n",
        "\n",
        "    @staticmethod\n",
        "    def draw_m_diagram(val_file, axes, title):\n",
        "        df = pd.read_csv(val_file)\n",
        "        b = 50  \n",
        "        hist = df[(df.REPURCHASE == 0) & (df.M <= 50000)].M.hist(bins=b, label='Non-Repurchase Customer',\n",
        "                                                                 ax=axes, color='blue', xrot=90)\n",
        "        df[(df.REPURCHASE == 1) & (df.M <= 50000)].M.hist(bins=b, label='Repurchase Customer',\n",
        "                                                          ax=axes, color='darkorange', xrot=90)\n",
        "        fd = {'fontsize': 18}\n",
        "        hist.set_title(title, fontdict=fd)\n",
        "        fd = {'fontsize': 16}\n",
        "        hist.set_xlabel('Monetary(1-50K)', fontdict=fd)\n",
        "        hist.set_ylabel('Customers', fontdict=fd)\n",
        "        hist.legend()\n",
        "        hist.set_xticks(np.arange(0, 100000, 5000))\n",
        "        hist.set_xlim(0, 50000)\n",
        "        hist.set_ylim(0, 20000)\n",
        "        count1, division1 = np.histogram(df[(df.REPURCHASE == 0) & (df.M <= 50000)].M, bins=b)\n",
        "        count2, division2 = np.histogram(df[(df.REPURCHASE == 1) & (df.M <= 50000)].M, bins=b)\n",
        "        df_d = pd.DataFrame({'Non-Repurchase Customer': count1, 'Repurchase Customer': count2})\n",
        "        df_transpose = df_d.T\n",
        "        return df_transpose\n",
        "\n",
        "    @staticmethod\n",
        "    def draw_venn(result_file, val_file, th, axes):\n",
        "        df1 = pd.read_csv(result_file)\n",
        "        df2 = pd.read_csv(val_file)\n",
        "        repurchase_set = df2[df2.REPURCHASE == 1]\n",
        "        predict_set = df1[df1['Pred'] >= th]\n",
        "        intersection_set = repurchase_set.merge(predict_set, on='ID', how='inner')\n",
        "        sr = len(repurchase_set)\n",
        "        sp = len(predict_set)\n",
        "        si = len(intersection_set)\n",
        "        v = venn2(subsets=(sr-si, sp-si, si), set_labels=('Reality Set={}'.format(sr), 'Predict Set={}'.format(sp)),\n",
        "                  ax=axes)\n",
        "        v.get_patch_by_id('10').set_color('red')\n",
        "        v.get_patch_by_id('01').set_color('blue')\n",
        "        v.get_patch_by_id('10').set_edgecolor('none')\n",
        "        v.get_patch_by_id('01').set_edgecolor('none')\n",
        "        v.get_patch_by_id('10').set_alpha(0.3)\n",
        "        v.get_patch_by_id('01').set_alpha(0.3)\n",
        "        for text in v.subset_labels:\n",
        "            if text:\n",
        "                text.set_fontsize(18)\n",
        "        if sr == 0 or sp == 0:\n",
        "            axes.set_title('Threshold={} ({}, {}, {}, Recall=?%, Precision=?%)'.format(th, sr-si, si, sp-si))\n",
        "        else:\n",
        "            axes.set_title('Threshold={} ({}, {}, {}, Recall={:.2%}, Precision={:.2%})'.format(\n",
        "                            th, sr-si, si, sp-si, si/sr, si/sp))\n",
        "        c = venn2_circles(subsets=(sr-si, sp-si, si), ax=axes)\n",
        "        r = c[0].get_radius() \n",
        "        axes.set_xlim(-4*r, 4*r)\n",
        "        r_max = 36*30  # 觀察三年\n",
        "        b = np.arange(1, r_max+30, 30)\n",
        "        count1, division1 = np.histogram(intersection_set.R_x, bins=b)\n",
        "        df_d = pd.DataFrame({'Predict-Repurchase Customer': count1})\n",
        "        df_transpose = df_d.T\n",
        "        return df_transpose\n",
        "\n",
        "    @staticmethod\n",
        "    def draw_stack(df_t1, df_t2, df_t3, axes):\n",
        "        d1 = np.array(df_t1.T) - np.array(df_t3.T)\n",
        "        d2 = np.array(df_t3.T)\n",
        "        d3 = np.array(df_t2.T) - np.array(df_t3.T)\n",
        "        dd = np.column_stack((d1, d2, d3))\n",
        "        df = pd.DataFrame(dd, columns=['FN', 'TP', 'FP'])\n",
        "        df.index = np.arange(1, 37, 1).astype('str')\n",
        "        df.plot(kind='bar', stacked=True, ax=axes, width=1, rot=0, color=['r', 'darkorange', 'b'], alpha=0.3)\n",
        "   \n",
        "    @staticmethod\n",
        "    def plot_threshold_precision(axes, report, name):\n",
        "        axes.plot(report['Threshold'], report['Precision'], lw=1.5, \n",
        "                  label='{0} Threshold/Precision'.format(name))\n",
        "        axes.legend()\n",
        "        fd = {'fontsize': 18}\n",
        "        axes.set_title('Threshold/Precision', fontdict=fd)\n",
        "        fd = {'fontsize': 16}\n",
        "        axes.set_xlabel('Threshold', fontdict=fd)\n",
        "        axes.set_ylabel('Precision', fontdict=fd)\n",
        "        axes.set_xlim([-0.05, 1.05])\n",
        "        axes.set_ylim([-0.05, 1.05])\n",
        "        axes.grid(True)\n",
        "        \n",
        "    @staticmethod\n",
        "    def plot_threshold_recall(axes, report, name):\n",
        "        axes.plot(report['Threshold'], report['Recall'], lw=1.5, \n",
        "                  label='{0} Threshold/Recall'.format(name))\n",
        "        axes.legend()\n",
        "        fd = {'fontsize': 18}\n",
        "        axes.set_title('Threshold/Recall', fontdict=fd)\n",
        "        fd = {'fontsize': 16}\n",
        "        axes.set_xlabel('Threshold', fontdict=fd)\n",
        "        axes.set_ylabel('Recall', fontdict=fd)\n",
        "        axes.set_xlim([-0.05, 1.05])\n",
        "        axes.set_ylim([-0.05, 1.05])\n",
        "        axes.grid(True)\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_precision_recall(axes, rea, pred, name):\n",
        "        precision, recall, thresholds = precision_recall_curve(rea, pred, pos_label=1)\n",
        "        axes.plot(recall, precision, lw=1.5, label='{0} Precision/Recall'.format(name))\n",
        "        axes.legend()\n",
        "        fd = {'fontsize': 18}\n",
        "        axes.set_title('Precision & Recall', fontdict=fd)\n",
        "        fd = {'fontsize': 16}\n",
        "        axes.set_xlabel('Recall', fontdict=fd)\n",
        "        axes.set_ylabel('Precision', fontdict=fd)\n",
        "        axes.set_xlim([-0.05, 1.05])\n",
        "        axes.set_ylim([-0.05, 1.05])\n",
        "        axes.grid(True)\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_roc(axes, rea, pred, name):\n",
        "        if not 'Baseline' in axes.get_legend_handles_labels()[1]:\n",
        "            axes.plot([0, 1], [0, 1], 'k--', lw=3, label='Baseline')\n",
        "        fpr, tpr, thresholds = roc_curve(rea, pred, pos_label=1)\n",
        "        axes.plot(fpr, tpr, lw=3, label='{0} ROC (AUC = {1:.2f})'.format(name, auc(fpr, tpr)))\n",
        "        axes.legend(loc=\"lower right\")\n",
        "        fd = {'fontsize': 18}\n",
        "        axes.set_title('ROC Curves', fontdict=fd)\n",
        "        fd = {'fontsize': 16}\n",
        "        axes.set_xlabel('False Positive Rate', fontdict=fd)\n",
        "        axes.set_ylabel('True Positive Rates', fontdict=fd)\n",
        "        axes.set_xlim([-0.05, 1.05])\n",
        "        axes.set_ylim([-0.05, 1.05])\n",
        "        axes.grid(True)\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_lift(axes, lift, name):\n",
        "        if not 'Baseline' in axes.get_legend_handles_labels()[1]:\n",
        "            axes.plot([0, 1], [0, 1], 'k--', lw=3, label='Baseline')\n",
        "        axes.plot(lift['%'], lift['Gain'], lw=3, label='{0}'.format(name))\n",
        "        axes.legend()\n",
        "        fd = {'fontsize': 18}\n",
        "        axes.set_title('A Cumulative Life Curve', fontdict=fd)\n",
        "        fd = {'fontsize': 16}\n",
        "        axes.set_xlabel('Percent of testing examples', fontdict=fd)\n",
        "        axes.set_ylabel('Percent of total responders', fontdict=fd)\n",
        "        axes.set_yticks(np.arange(0, 1.1, 0.1))\n",
        "        axes.set_yticklabels(np.arange(0, 110, 10))\n",
        "        axes.set_xticks(np.arange(0, 1.1, 0.1))\n",
        "        axes.set_xticklabels(np.arange(0, 110, 10))\n",
        "        axes.grid(True)\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_response(axes, lift, name, y_max='default', marketing_num = 12000):\n",
        "        axes.plot(lift['Cost'], lift['Response'], lw=1.5, label='{0} Response'.format(name))\n",
        "        fd = {'fontsize': 18}\n",
        "        axes.set_title('Cost & Response', fontdict=fd)\n",
        "        fd = {'fontsize': 16}\n",
        "        axes.set_xlabel('Cost Num', fontdict=fd)\n",
        "        axes.set_ylabel('Response Num', fontdict=fd)\n",
        "        if not y_max=='default':\n",
        "            axes.set_ylim([0, y_max])\n",
        "        total_count = np.max(lift['Cost'])\n",
        "        axes.set_xticks(np.arange(0, total_count, marketing_num))\n",
        "        vals = axes.get_xticks()\n",
        "        axes.set_xticklabels(['{0:.0f}K'.format(x/1000) for x in vals], rotation=40)\n",
        "        axes.grid(True)\n",
        "        \n",
        "    @staticmethod    \n",
        "    def get_cumulative(Result, val_file):\n",
        "        pred = Result['Pred']\n",
        "        val = pd.read_csv(val_file)\n",
        "        rea = val.loc[:, ['REPURCHASE']].values\n",
        "        repurchase = len(val[val.REPURCHASE == 1])\n",
        "        total_test = len(val)\n",
        "        Result.sort_values(ascending=False, inplace=True, by='Pred')\n",
        "        Result = Result.reset_index(drop=True)\n",
        "        Lift = pd.DataFrame()\n",
        "        rate = 0.025 #6000/total_test\n",
        "        for percentage in np.arange(0, 1+rate, rate):\n",
        "            cost = int(total_test*percentage)\n",
        "            response = len(val[val.REPURCHASE == 1].merge(Result.loc[:cost-1, :],\n",
        "                                                          on='ID', how='inner'))\n",
        "            pd.options.display.float_format = '{0:.4f}'.format\n",
        "            if percentage == 0:\n",
        "                Lift = Lift.append({'%':0,\n",
        "                                    'Cost': 0,\n",
        "                                    'Response': 0,\n",
        "                                    'Recall': 0,  # Gain與Recall一樣\n",
        "                                    'Lift': 0,\n",
        "                                    'Precision': 0},  ignore_index=True)\n",
        "            else:\n",
        "                Lift = Lift.append({'%': percentage,\n",
        "                                    'Cost': cost,\n",
        "                                    'Response': response,\n",
        "                                    'Recall': response/repurchase,  # Gain與Recall一樣\n",
        "                                    'Lift': (response/repurchase)/percentage,\n",
        "                                    'Precision': response/cost}, ignore_index=True)\n",
        "        return Lift\n",
        "\n",
        "    @staticmethod\n",
        "    def get_lift_index(Result, val_file):\n",
        "        lift = Diagram.get_cumulative(Result, val_file)\n",
        "        alift = lift['Recall'].iloc[1:].reset_index(drop=True)\n",
        "        blift = lift['Recall'].iloc[:20]\n",
        "        p1 = pd.Series(np.arange(1, 0, -0.05))\n",
        "        p2 = alift.sub(blift)\n",
        "        li = np.sum(p1*p2)\n",
        "        return li\n",
        "\n",
        "    @staticmethod    \n",
        "    def get_section(Result, val_file):\n",
        "        lift = Diagram.get_cumulative(Result, val_file)\n",
        "        p3 = lift.loc[:, ['%', 'Cost', 'Recall', 'Response']]\n",
        "        clift = p3.iloc[1:].reset_index(drop=True)\n",
        "        dlift = p3.iloc[:40]\n",
        "        p3 = clift.sub(dlift)\n",
        "        p3.insert(loc=2, column='Precision', value=pd.Series(p3.Response / p3.Cost))\n",
        "        return p3\n",
        "\n",
        "    @staticmethod\n",
        "    def estimate(train_file, val_file, marketing_num=12000, model_name='Forest', seed=0, features=['R', 'F', 'M', 'RB', 'FB', 'MB']):\n",
        "        if model_name == 'Forest':\n",
        "            Result = Forest.get_result(train_file, val_file, seed)\n",
        "        elif model_name == 'Naive':\n",
        "            Result = Naive.get_result(train_file, val_file)\n",
        "        else:\n",
        "            Result = SVM.get_result(train_file, val_file)\n",
        "\n",
        "        pred = Result['Pred']\n",
        "        val = pd.read_csv(val_file)\n",
        "        \n",
        "        Report = pd.DataFrame()\n",
        "        rea = val.loc[:, ['REPURCHASE']].values\n",
        "        # Lift Chart\n",
        "        repurchase = len(val[val.REPURCHASE == 1])\n",
        "        total_test = len(val)\n",
        "        Result.sort_values(ascending=False, inplace=True, by='Pred')\n",
        "        Result = Result.reset_index(drop=True)\n",
        "        Lift = pd.DataFrame()\n",
        "        for percentage in np.arange(0, 1.1, 0.1):\n",
        "            cost = int(total_test*percentage)\n",
        "            response = len(val[val.REPURCHASE == 1].merge(Result.loc[:cost-1, :],\n",
        "                                                          on='ID', how='inner'))\n",
        "            pd.options.display.float_format = '{0:.2f}'.format\n",
        "            Lift = Lift.append({'%': percentage,\n",
        "                                'Cost': cost,\n",
        "                                'Response': response,\n",
        "                                'Gain': response/repurchase,\n",
        "                                'Lift': response/repurchase}, ignore_index=True)\n",
        "        Response = pd.DataFrame()\n",
        "        total_members = len(Result)\n",
        "\n",
        "        for cost in np.arange(marketing_num, total_members, marketing_num):\n",
        "            re_list = val[val.REPURCHASE == 1].merge(Result.loc[:cost-1, :],\n",
        "                                                     on='ID', how='inner')\n",
        "            response = len(re_list)\n",
        "            income = re_list['AMOUNT'].sum()\n",
        "            mean = re_list['AMOUNT'].mean()\n",
        "            std = re_list['AMOUNT'].std()\n",
        "            pd.options.display.float_format = '{0:.2f}'.format\n",
        "            Response = Response.append({\n",
        "                                'Cost': cost,\n",
        "                                'Response': response,\n",
        "                                'Income': income,\n",
        "                                'Mean': mean,\n",
        "                                'Std': std}, ignore_index=True)\n",
        "        cost = len(Result)\n",
        "        re_list = val[val.REPURCHASE == 1]\n",
        "        response = len(re_list)\n",
        "        income = re_list['AMOUNT'].sum()\n",
        "        mean = re_list['AMOUNT'].mean()\n",
        "        std = re_list['AMOUNT'].std()\n",
        "        pd.options.display.float_format = '{0:.2f}'.format\n",
        "        Response = Response.append({\n",
        "                            'Cost': cost,\n",
        "                            'Response': response,\n",
        "                            'Income': income,\n",
        "                            'Mean': mean,\n",
        "                            'Std': std}, ignore_index=True)\n",
        "        for th in np.arange(0, 1.0, 0.1):\n",
        "            pre = (pred >= th).astype(int).values\n",
        "            tn, fp, fn, tp = confusion_matrix(rea, pre).ravel()\n",
        "            Report = Report.append({'Threshold': th,\n",
        "                                    'Accuracy': accuracy_score(rea, pre),\n",
        "                                    'Precision': precision_score(rea, pre),\n",
        "                                    'Recall': recall_score(rea, pre),\n",
        "                                    'Total_Predict': (tp+fp),\n",
        "                                    'TP': tp,\n",
        "                                    'FP': fp,\n",
        "                                    'TN': tn,\n",
        "                                    'FN': fn}, ignore_index=True)\n",
        "        Report['AUC'] = \"{0:.2f}%\".format(roc_auc_score(rea, pred) * 100)\n",
        "        return Report, Lift, rea, pred, Response\n",
        "\n",
        "print('done', time.time())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done 1617415813.608456\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "phtBed4nwiov",
        "outputId": "d58aa1e1-6941-4479-d401-07743587ae25"
      },
      "source": [
        "i_file = '/content/drive/Shareddrives/louvain/test.csv'\n",
        "p_date_list = [['2016-10-01', '2016-11-30']]\n",
        "\n",
        "p_date_list.sort()\n",
        "t_date_list = ['2016-01-04']\n",
        "\n",
        "features = ['R', 'F', 'M', 'L', 'WNPT', 'CAI', 'CCC']\n",
        "\n",
        "for p_date in p_date_list:\n",
        "    for t_date in t_date_list:\n",
        "        rfm = RFM(i_file, p_date[1], p_date[0], t_s_date=t_date)\n",
        "        info_file = rfm.get_info()\n",
        "        print('gen info_file: {0}'.format(info_file))\n",
        "    \n",
        "        train_file = rfm.gen_train(oversampling=4, features=features)\n",
        "        print('gen train_file: {0}'.format(train_file))\n",
        "        \n",
        "        # test與val檔二選一，test未得預測交易，val已得預測交易\n",
        "        test_file = rfm.gen_test(features=features)\n",
        "        print('gen test_file: {0}'.format(test_file))\n",
        "\n",
        "        val_file = rfm.gen_val(features=features)\n",
        "        print('gen val_file: {0}'.format(val_file))\n",
        "        \n",
        "        features = ['R', 'F', 'M', 'RB', 'FB', 'MB', 'L', 'CAI']\n",
        "        result_file = Forest.gen_result(train_file, test_file, o_file='20070311_20200715-0920_asc_result_L_CAI.csv', features=features)\n",
        "        print('gen result_file: {0}'.format(result_file))\n",
        "        \n",
        "print('done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gen info_file: 20160104_20161001-1130_asc_info.csv\n",
            "gen train_file: 20160104_20161001-1130_asc_train_o4.csv\n",
            "gen test_file: 20160104_20161001-1130_asc_test.csv\n",
            "gen val_file: 20160104_20161001-1130_asc_val.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-61f18cf917cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'R'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'F'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'M'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CAI'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mresult_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mForest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'20070311_20200715-0920_asc_result_L_CAI.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gen result_file: {0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-8babf628defd>\u001b[0m in \u001b[0;36mgen_result\u001b[0;34m(train_file, test_file, o_file, seed, features)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mo_file\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mo_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'result'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mForest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mo_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-8babf628defd>\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(train_file, test_file, seed, features)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0mResult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Pred'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mResult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    871\u001b[0m                     \u001b[0;31m# AttributeError for IntervalTree get_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1444\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Too many indexers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_key\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m             \u001b[0;31m# a tuple should already have been caught by this point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1435\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1437\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[0;31m# -------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
          ]
        }
      ]
    }
  ]
}